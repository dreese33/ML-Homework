# hw1.py

import os.path
import numpy

######################################
#
# FUNCTIONS YOU WILL NEED TO MODIFY:
#  - linreg_closed_form
#  - loss
#  - linreg_grad_desc
#  - random_fourier_features
#
######################################

files_array = ["1D-exp-samp.txt", "1D-exp-uni.txt", "1D-no-noise-lin.txt", "1D-quad-uni-noise.txt",
               "1D-quad-uni.txt", "2D-noisy-lin.txt"]

def linreg_model_sample(Theta, model_X):
	if model_X.shape[1] == 1:

		## get a bunch of evenly spaced X values in the same range as the passed in data

		sampled_X = numpy.linspace(model_X.min(axis=0), model_X.max(axis=0), 100)

		## get the Y values for our sampled X values by taking the dot-product with the model
		## Note: we're appending a column of all ones so we can do this with a single matrix-vector multiply

		sampled_Y = numpy.hstack([numpy.ones((sampled_X.shape[0], 1)), sampled_X]).dot(Theta)
		return sampled_X, sampled_Y
	elif model_X.shape[1] == 2:
		## Unfortunately, plotting surfaces is a bit more complicated, first we need
		## a set of points that covers the area we want to plot. numpy.meshgrid is a helper function
		## that will create two NxN arrays that vary over both the X and Y range given.

		sampled_X, sampled_Y = numpy.meshgrid(model_X[:, 0], model_X[:, 1])

		## We can't just do a simple matrix multiply here, because plot_surface(...) is going to expect NxN arrays like
		## those generated by numpy.meshgrid(...). So here we're explicitly pulling out the components of Theta as
		## scalars and multiplying them across each element in the X and Y arrays to get the value for Z

		sampled_Z = sampled_X*Theta[1]+sampled_Y*Theta[2]+Theta[0]
		return sampled_X, sampled_Y, sampled_Z

def plot_helper(data_X, data_Y, model_X=None, model_Y=None, model_Z=None):
	import matplotlib.pyplot

	## 2D plotting
	## data_X.shape[1] is the number of columns in data_X, just as data_X.shape[0] is the number of rows

	if data_X.shape[1] == 1:
		fig1 = matplotlib.pyplot.figure() ## creates a new figure object that we can plot into
		fig1.gca().scatter(data_X,data_Y) ## creates a scatterplot with the given set of X and Y points

		## If we were given a model, we need to plot that

		if not(model_X is None) and not(model_Y is None):
			## Plot the data from the model
			## Note: we're using plot(...) instead of scatter(...) because we want a smooth curve

			fig1.gca().plot(model_X, model_Y, color='r')

		## The graph won't actually be displayed until we .show(...) it. You can swap this with savefig(...) if you
		## instead want to save an image of the graph instead of displaying it. You can also use the interface to save an
		## image after displaying it
		matplotlib.pyplot.show() #fig1.show()
	## 3D plotting
	elif data_X.shape[1] == 2:

		## This import statement 'registers' the ability to do 3D projections/plotting with matplotlib
		from mpl_toolkits.mplot3d import Axes3D

		fig1 = matplotlib.pyplot.figure()

		## The format for 3D scatter is similar to 2D; just add the third dimension to the argument list

		fig1.gca(projection='3d').scatter(data_X[:,0],data_X[:,1],data_Y)
		if not(model_X is None) and not(model_Y is None) and not(model_Z is None):

			## Now, with our X, Y, and Z arrays (all NxN), we can use plot_surface(...) to create a nice 3D surface

			fig1.gca(projection='3d').plot_surface(model_X, model_Y, model_Z,linewidth=0.0,color=(1.0,0.2,0.2,0.75))
		matplotlib.pyplot.show() #fig1.show()
	else:

		## Matplotlib does not yet have the capability to plot in 4D

		print('Data is not in 2 or 3 dimensions, cowardly refusing to plot! (data_X.shape == {})'.format(data_X.shape))

## Data loading utility function

def load_data(fname, directory='data'):
	data = numpy.loadtxt(os.path.join(directory, fname), delimiter=',')
	rows, cols = data.shape
	X_dim = cols-1
	Y_dim = 1
	return data[:, :-1].reshape(-1, X_dim), data[:, -1].reshape(-1, Y_dim)


def vis_linreg_model(train_X, train_Y, Theta):
	if Theta.shape[0] == 2:
		sample_X, sample_Y = linreg_model_sample(Theta,train_X)
		#NOTE: this won't work directly with 3D data. Write your own function, or modify this one
        #to generate plots for 2D-noisy-lin.txt or other 3D data.
		plot_helper(train_X, train_Y, sample_X, sample_Y)
	else:
		sample_X, sample_Y, sample_Z = linreg_model_sample(Theta, train_X)
		plot_helper(data_X=train_X, data_Y=train_Y, model_X=sample_X, model_Y=sample_Y, model_Z=sample_Z)

###################
# YOUR CODE BELOW #
###################

def linreg_closed_form(train_X, train_Y):
	'''
	Computes the optimal parameters for the given training data in closed form
	Args:
		train_X (N-by-D numpy array): Training data features as a matrix of row vectors (train_X[i][j] is the jth component of the ith example)
		train_Y (length N numpy array): The training data target as a length N vector
	Returns:
		A length D+1 numpy array with the optimal parameters
	'''
	x_transpose = numpy.transpose(train_X)
	x_transpose_dot = numpy.dot(x_transpose, train_X)
	inverse_val = numpy.linalg.inv(x_transpose_dot)
	Theta = numpy.dot(numpy.dot(inverse_val, x_transpose), train_Y)
	ret_val = numpy.array(numpy.vstack([0.0, Theta]))
	return ret_val

###################
# YOUR CODE BELOW #
###################
def loss(Theta, train_X, train_Y):
	'''
	Computes the squared loss for the given setting of the parameters given the training data
	Args:
		Theta (length D+1 numpy array): the parameters of the model
		train_X (N-by-D numpy array): Training data features as a matrix of row vectors (train_X[i][j] is the jth component of the ith example)
		train_Y (length N numpy array): The training data target as a length N vector
	Returns:
		The (scalar) loss for the given parameters and data.
	'''
	rv = None
	n = train_X.shape[0]
	for i in range(n):
		val = (train_X[i].dot(Theta) - train_Y[i])
		val_transpose = numpy.transpose(val)
		total = val_transpose.dot(val)
		if i == 0:
			rv = total
		else:
			rv += total
	rv /= 2 * n
	return rv


###################
# YOUR CODE BELOW #
###################
def linreg_grad_desc(initial_Theta, train_X, train_Y, alpha=0.05, num_iters=500, print_iters=True):
	'''
	Fits parameters using gradient descent
	Args:
		initial_Theta ((D+1)-by-1 numpy array): The initial value for the parameters we're optimizing over
		train_X (N-by-D numpy array): Training data features as a matrix of row vectors (train_X[i][j] is the jth component of the ith example)
		train_Y (N-by-1 numpy array): The training data target as a vector
		alpha (float): the learning rate/step size, defaults to 0.1
		num_iters (int): number of iterations to run gradient descent for, defaults to 500
	Returns:
		The history of theta's and their associated loss as a list of tuples [ (Theta1,loss1), (Theta2,loss2), ...]
	'''

	train_X = numpy.hstack([numpy.ones((train_X.shape[0], 1)), train_X])
	cur_Theta = initial_Theta
	step_history = list()

	x_transpose = numpy.transpose(train_X)
	term1 = x_transpose.dot(train_X)
	term2 = x_transpose.dot(train_Y)

	for k in range(1, num_iters + 1):
		cur_loss = loss(cur_Theta, train_X, train_Y)
		step_history.append((cur_Theta, cur_loss))
		if print_iters:
			print("Iteration: {} , Loss: {} , Theta: {}".format(k, cur_loss, cur_Theta))

		term1_final = term1.dot(cur_Theta)
		cur_Theta = cur_Theta - alpha * (1 / (train_Y.shape[0])) * (term1_final - term2)

	return numpy.asarray(step_history)


def apply_RFF_transform(X, Omega, B):
	'''
	Transforms features into a Fourier basis with given samples
		Given a set of random inner products and translations, transform X into the Fourier basis, Phi(X)
			phi_k(x) = cos(<x,omega_k> + b_k)                           #scalar form
			Phi(x) = sqrt(1/D)*[phi_1(x), phi_2(x), ..., phi_NFF(x)].T  #vector form
			Phi(X) = [Phi(x_1), Phi(x_2), ..., Phi(x_N)].T              #matrix form
	Args:
		X (N-by-D numpy array): matrix of row-vector features (may also be a single row-vector)
		Omega (D-by-NFF numpy array): matrix of row-vector inner products
		B (NFF length numpy array): vector of translations
	Returns:
		A N-by-NFF numpy array matrix of transformed points, Phi(X)
	'''
	return numpy.sqrt(1.0 / Omega.shape[1]) * numpy.cos(X.dot(Omega) + B)

##################
# YOUR CODE HERE #
##################
def random_fourier_features(train_X, train_Y, num_fourier_features=100, alpha=0.1, num_iters=500, print_iters=False):
	'''
	Creates a random set of Fourier basis functions and fits a linear model in this space.
		Randomly sample num_fourier_features's non-linear transformations of the form:
			phi_k(x) = cos(<x,omega_k> + b_k)
			Phi(x) = sqrt(1/D)*[phi_1(x), phi_2(x), ..., phi_NFF(x)]
		where omega_k and b_k are sampled according to (Rahimi and Recht, 20018).
	Args:
		train_X (N-by-D numpy array): Training data features as a matrix of row vectors (train_X[i][j] is the jth component of the ith example)
		train_Y (length N numpy array): The training data target as a length N vector
		num_fourier_features (int): the number of random features to generate
	Returns:
		Theta (numpy array of length num_fourier_features+1): the weights for the *transformed* model
		Omega (D-by-num_fourier_features numpy array): the inner product term of the transformation
		B (numpy array of length num_fourier_features): the translation term of the transformation
	'''

	identity_matrix = numpy.eye(num_fourier_features)
	mean = numpy.zeros(num_fourier_features)
	Omega = numpy.random.multivariate_normal(mean, identity_matrix, train_X.shape[1])
	B = (numpy.random.random((num_fourier_features,)) - 0.5) * 0.2
	Phi = apply_RFF_transform(train_X, Omega, B)

	initial_Theta = (numpy.random.random(size=(num_fourier_features + 1, 1)) - 0.5) * 0.2
	step_history = linreg_grad_desc(initial_Theta, Phi, train_Y, alpha=alpha, num_iters=num_iters, print_iters=print_iters)

	print("Step History: ")
	print(step_history[-1][0])

	return step_history[-1][0], Omega, B


def rff_model_sample(Theta, Omega, B, model_X):
	sampled_X = numpy.linspace(model_X.min(axis=0), model_X.max(axis=0), 100)
	Phi = apply_RFF_transform(sampled_X, Omega, B)

	Phi = numpy.hstack([numpy.array([numpy.ones(Phi.shape[0])]).T, Phi])
	sampled_Y = Phi.dot(Theta)
	return sampled_X, sampled_Y


def vis_rff_model(train_X, train_Y, Theta, Omega, B):
	sample_X, sample_Y = rff_model_sample(Theta, Omega,B, train_X)
	plot_helper(train_X, train_Y, sample_X, sample_Y)


'''
	Tests section, run these to see function outputs:
'''


'''
Test this function for indexes 2, and 5, for question 1a

Params:
	- index - The index of the data file from the files_array
	- duplicate_column - Copies and adds a column to demonstrate for 1b
	- duplicate_row - Copies and adds a row to demonstrate for 1c

Outputs: Closed form theta, and closed form loss. Plot of closed form.
'''

def test_loss_linreg(index, duplicate_column=False, duplicate_row=False):
	data_X, data_Y = load_data(files_array[index])
	data_X = numpy.hstack([numpy.ones((data_X.shape[0], 1)), data_X])
	if duplicate_column:
		data_X = numpy.hstack([numpy.ones((data_X.shape[0], 1)), data_X])


	if duplicate_row:
		print(data_X.shape)
		data_X = numpy.vstack([data_X[2], data_X])
		data_Y = numpy.vstack([data_Y[2], data_Y])
		print("After", data_X.shape)

	theoretical_theta, theoretical_loss, _, _ = numpy.linalg.lstsq(data_X, data_Y, rcond=1)
	theoretical_loss = theoretical_loss / (2 * data_Y.shape[0])

	Theta = linreg_closed_form(data_X, data_Y)
	Theta = numpy.asarray([numpy.delete(Theta, [0])])
	Loss = loss(Theta.T, data_X, data_Y)

	print("Closed Form Theta:", Theta)
	#print("Theoretical Theta: ", theoretical_theta)

	print("Closed Form Loss:", Loss)
	#print("Theoretical Loss:", theoretical_loss)

	if index != 5:
		data_X = numpy.delete(data_X, 0, 1)
		vis_linreg_model(data_X, data_Y, Theta[0])
	else:
		data_X = numpy.delete(data_X, 0, 1)
		vis_linreg_model(data_X, data_Y, Theta[0])


'''
Tester function for part 2 of the homework

Params:
	- index - The index of the data file from the files_array
	- alpha, num_iters, print_iters - for linreg_grad_desc function
	- duplicate_column - Copies and adds a column to demonstrate for 1d
	- duplicate_row - Copies and adds a row to demonstrate for 1d

Outputs: Gradient descent theta, gradient descent loss, closed form theta, 
		closed form loss, plot of data for gradient descent.
'''

def test_gradient_descent(index, alpha=0.05, num_iters=500, print_iters=True, duplicate_column=False, duplicate_row=False):
	data_X, data_Y = load_data(files_array[index])
	initial_theta = [[0.], [0.]]
	if index == 5:
		initial_theta = [[0.], [0.], [0.]]

	if duplicate_column:
		data_X = numpy.hstack([numpy.ones((data_X.shape[0], 1)), data_X])
		initial_theta.append([0.])


	if duplicate_row:
		data_X = numpy.vstack([data_X[2], data_X])
		data_Y = numpy.vstack([data_Y[2], data_Y])

	theta = linreg_grad_desc(initial_theta, data_X, data_Y, alpha=alpha, num_iters=num_iters, print_iters=print_iters)
	loss1 = (theta[len(theta) - 1])[1]
	theta = (theta[len(theta) - 1])[0]

	print()
	print("grad_desc_theta: " + str(theta))
	print("grad_desc_loss: " + str(loss1))

	if duplicate_column:
		data_X = numpy.delete(data_X, 0, 1)
		if index == 5:
			theta = numpy.vstack([theta[1], theta[2], theta[3]])
		else:
			theta = numpy.vstack([theta[1], theta[2]])

	if index == 5:
		vis_linreg_model(data_X, data_Y, theta)
	else:
		vis_linreg_model(data_X, data_Y, theta)

	data_X = numpy.hstack([numpy.ones((data_X.shape[0], 1)), data_X])
	theta_closed = linreg_closed_form(data_X, data_Y)
	theta_closed = numpy.asarray([numpy.delete(theta_closed, [0])])
	loss2 = loss(theta_closed.T, data_X, data_Y)

	print()
	print("theoretical_theta: " + str(theta_closed))
	print("theoretical_loss: " + str(loss2))


def test_rff(index, num_fourier_features=100):
	data_X, data_Y = load_data(files_array[index])
	Theta, Omega, B = random_fourier_features(data_X, data_Y, num_fourier_features=num_fourier_features)
	vis_rff_model(data_X, data_Y, Theta, Omega, B)


'''
Note: files_array indexes:
0 - `1D-exp-samp.txt` 
1 - `1D-exp-uni.txt`
2 - `1D-no-noise-lin.txt`
3 - `1D-quad-uni-noise.txt`
4 - `1D-quad-uni.txt`
5 - `2D-noisy-lin.txt`
'''

# Main function
if __name__ == '__main__':
	#data_X, data_Y = load_data(files_array[2])
	index = 2
	#test_gradient_descent(index, alpha=0.01, num_iters=100000, print_iters=False)
	#test_rff(4, num_fourier_features=800)
	#test_loss_linreg(index)
